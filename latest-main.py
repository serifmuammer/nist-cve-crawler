import json
from datetime import datetime
import requests
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor

MAX_THREADS = 30

high_severity_count = 0

def getPageContent(url):
    return requests.get(url)


def getSoupData(page_content):
    return BeautifulSoup(page_content, 'html.parser')


# Generate last  three months urls
def generateLast3MonthsURLs():
    print(f'Generating last 3 months URLs...')
    # Generate last 3 month URL
    for i in range(3):
        if current_month - i <= 0:
            year = current_year - 1
            month = current_month - i + 12
        else:
            month = current_month - i
            year = current_year

        urls.append(f'{listing_url}{year}/{month}')
        i += 1
    print(f'URLs generated.')


def getCVEList(url_list):
    for url in url_list:
        page = getPageContent(url)
        cve_list_page_soup = getSoupData(page.text)
        cve_list.append(cve_list_page_soup.select('span.col-md-2'))

    return cve_list


def getCVELink(cve):
    tmp_link_path = cve.find('a', href=True)['href']
    cve_link = base_url + tmp_link_path
    return cve_link


def getSeverity(soup_page):
    severity = soup_page.find(id="Cvss3NistCalculatorAnchor")
    if severity is None:
        return 'none'
    else:
        return severity.text.strip()


def checkSeverity(text):
    if text == 'none':
        return False
    elif text.split()[1] == 'HIGH':
        return True
    else:
        return False


def getScore(text):
    return float(text.split()[0])

def checkCVEList(cve):
    tmp_dict = {}
    global high_severity_count
    cve_link = getCVELink(cve)
    # make a request to this link
    # get soup object and parse for this cve
    cve_details_page = getSoupData(getPageContent(cve_link).text)
    severity = getSeverity(cve_details_page)
    print(cve_link)
    print(cve_details_page)
    if checkSeverity(severity):
        # store cve information
        # cve-id, score, title, cve_link, summary
        high_severity_count += 1
        cve_id = cve_link.split('/')[5]
        score = getScore(severity)
        title = cve_details_page.select('title')
        title = str(title).split('<')[1].split('>')[1]  # to remove <title></tile> tags
        summary = cve_details_page.find('p', attrs={'data-testid': 'vuln-description'}).text
        print('Save CVE information to temporary dictionary.')
        tmp_dict['cve_id'] = cve_id
        tmp_dict['score'] = score
        tmp_dict['title'] = title
        tmp_dict['summary'] = summary
        list_of_cve_dict.append(tmp_dict)

current_year = datetime.today().year
current_month = datetime.today().month
listing_url = 'https://nvd.nist.gov/vuln/full-listing/'
base_url = 'https://nvd.nist.gov'
urls = []
cve_list = []
list_of_cve_dict = []

start_time = datetime.now()
print('Starting point')
generateLast3MonthsURLs()
print(urls)
cve_list = getCVEList(urls)  ### cve_list[0] -> urls[0]
total_cve_count = len(cve_list[0]) + len(cve_list[1]) + len(cve_list[2])
print(f'{total_cve_count} CVE found, checking details.')

#for cve in cve_list[0]:
#    checkCVEList(cve)

with ThreadPoolExecutor(max_workers=10) as executor:
    executor.map(checkCVEList, cve_list[0])

with ThreadPoolExecutor(max_workers=10) as executor2:
    executor2.map(checkCVEList, cve_list[1])

with ThreadPoolExecutor(max_workers=10) as executor3:
    executor3.map(checkCVEList, cve_list[2])
print('check severity')


print(f'{high_severity_count} HIGH severity found in total {total_cve_count} CVE')

sorted_cve_list = sorted(list_of_cve_dict, key=lambda score: float(score['score']))

with open('CVE_List.log', 'w+') as logfile:
    logfile.write('')
    for line in sorted_cve_list:
        logfile.write(json.dumps(line))
        logfile.write('\n')

finish_time = datetime.now()
exec_time = finish_time - start_time
print(f'Execution time: {exec_time}')
