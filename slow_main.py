import json
from datetime import datetime
import requests
from bs4 import BeautifulSoup

def getPageSoup(url):
    page = requests.get(url, stream=True)
    return BeautifulSoup(page.text, 'html.parser')

def generateURLs():
    urls = []
    print(f'Generating last 3 months URLs...')
    # Generate last 3 month URL
    for i in range(3):
        if current_month - i <= 0:
            year = current_year - 1
            month = current_month - i + 12
        else:
            month = current_month - i
            year = current_year

        urls.append(f'{listing_url}{year}/{month}')
        i += 1
    print(f'URLs generated.')
    return urls


start_time = datetime.now()
print(start_time)

base_url = 'https://nvd.nist.gov'
listing_url = 'https://nvd.nist.gov/vuln/full-listing/'
urls = []
cve_dict_list = []
total_cve = 0
high_count = 0
current_year = datetime.today().year
current_month = datetime.today().month
# current_month = 1
# 1-> 11 12 1
# 2-> 12 1 2
# 3-> 1 2 3
# 4-> 2 3 4

# Overwrite to urls list for taking just 1 month result. Commenting next line will allow to execute for last 3 months.
# urls = generateURLs()
urls = ['https://nvd.nist.gov/vuln/full-listing/2022/2']

for url in urls:

    soup_data = getPageSoup(url)
    cves = soup_data.select(".col-md-2")
    total_cve += len(cves)
    print(f'Total {len(cves)} entries found on {url}. ')
    print('Severity check running for all found CVE...')
    i = 1
    for cve in cves:
        # request all cve and look for severity
        dict_tmp = {}
        severity_high = False
        cve_link_tmp = cve.find('a', href=True)['href']
        cvss_url = base_url + cve_link_tmp
        cvss_score_page_soup = getPageSoup(cvss_url)
        # CVSS V3 score id=Vuln3CvssPanel
        try:
            severity_detail = cvss_score_page_soup.find(id="Vuln3CvssPanel").select('.severityDetail')
        except:
            pass
        # Some vulnerabilities have more than 1 CVSS v3 score. Checking all off possible scores and control any of them equal to HIGH
        # For example CVE-2020-35391
        # NIST: NVD Base Score:  6.5 MEDIUM and  CNA:  MITRE Base Score:  9.6 CRITICAL
        for severity in severity_detail:
            try:
                severity_txt = severity.text.strip().split()[1]

                if severity_txt == 'HIGH':
                    severity_high = True
                    print(f'{i}. Running...')
                    score = float(severity.text.strip().split()[0])
                    vuln_description: str = 'Current Description\n' + cvss_score_page_soup.find('p', attrs={'data-testid': 'vuln-description'}).text
                    vuln_quick_info: str = cvss_score_page_soup.find('div', attrs={'class': 'bs-callout bs-callout-info'}).text
                    summary = vuln_description + vuln_quick_info
                    title = cvss_score_page_soup.select("title")
                    title = str(title).split('<')[1].split('>')[1]  # to remove <title></tile> tags
                    cve_id = cve_link_tmp.split('/')[3]

            except:
                score = ''
                severity_txt = ''
                pass

        if severity_high:
            high_count += 1
            # Store data in dict
            dict_tmp['cve-id'] = cve_id
            dict_tmp['score'] = score
            dict_tmp['title'] = title
            dict_tmp['link'] = cvss_url
            dict_tmp['summary'] = summary
            cve_dict_list.append(dict_tmp)
        i += 1
print(f'All CVE severity checks completed. High:{high_count} Total: {total_cve} ')

sorted_cve_list = sorted(cve_dict_list, key=lambda score: float(score['score']))

with open('cves_slow.log', 'w+') as logfile:
    logfile.write('')
    for line in sorted_cve_list:
        logfile.write(json.dumps(line))
        logfile.write('\n')

finish_time = datetime.now()
print(finish_time)
exec_time = finish_time - start_time
print(f'Execution time: {exec_time}')