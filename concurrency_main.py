import concurrent.futures
import json
from datetime import datetime
import requests
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor

def generateUrls():
    urls=[]
    print(f'Generating last 3 months URLs...')
    # Generate last 3 month URL
    for i in range(3):
        if current_month - i <= 0:
            year = current_year - 1
            month = current_month - i + 12
            # print(f'Month/Year:{month}/{year}')
        else:
            month = current_month - i
            year = current_year
            # print(f'Month/Year:{month}/{year}')

        urls.append(f'{listing_url}{year}/{month}')
        i += 1
    print(f'URLs generated.')
    return urls

def get_url(url):
    return requests.get(url)

def getPageSoup(url):
    page = requests.get(url)
    return BeautifulSoup(page.text, 'html.parser')


def getCVElink(soup):
    cve_link_tmp = soup.find('a', href=True)['href']
    cvss_url = base_url + cve_link_tmp
    cvss_url_list.append(cvss_url)

start_time = datetime.now()
base_url = 'https://nvd.nist.gov'
listing_url = 'https://nvd.nist.gov/vuln/full-listing/'
cvss_url_list = []
current_year = datetime.today().year
current_month = datetime.today().month
cve_dict_list = []
total_cve = 0
high_count = 0

# Overwrite to urls list for taking just 1 month result.
#urls = ['https://nvd.nist.gov/vuln/full-listing/2022/2']
urls = generateUrls()

for url in urls:

    soup_data = getPageSoup(url)
    cves = soup_data.select(".col-md-2")
    total_cve += len(cves)
    print(f'Total {len(cves)} entries found on {url}. ')
    i = 1
    #for cve in cves:
        # request all cve and look for severity
    with concurrent.futures.ThreadPoolExecutor(max_workers=20) as requester:
        requester.map(getCVElink, cves)

print(f'Total {len(cvss_url_list)} CVE...')

list_of_urls = cvss_url_list

with ThreadPoolExecutor(max_workers=10) as pool:
    response_list = list(pool.map(get_url, list_of_urls))

def getsoup(response_text):
    return BeautifulSoup(response.text, 'html.parser')


def check_severity(response_soup):
    dict_tmp = {}
    i = 0
    severity_high = False
    cvss_score_page_soup = getsoup(response.text)

    try:
        severity_detail = cvss_score_page_soup.find(id="Vuln3CvssPanel").select('.severityDetail')
    except:
        pass
    # Some vulnerabilities have more than 1 CVSS v3 score. Checking all off possible scores and control any of them equal to HIGH
    # For example CVE-2020-35391
    # NIST: NVD Base Score:  6.5 MEDIUM and  CNA:  MITRE Base Score:  9.6 CRITICAL
    for severity in severity_detail:
        severity = severity.text
        try:
            if severity.split()[1] == 'HIGH':
                severity_high = True
                print(f'{i}. Running...')
                vuln_description: str = 'Current Description\n' + cvss_score_page_soup.find('p', attrs={
                    'data-testid': 'vuln-description'}).text
                vuln_quick_info: str = cvss_score_page_soup.find('div',
                                                                 attrs={'class': 'bs-callout bs-callout-info'}).text
                summary = vuln_description + vuln_quick_info
                title = cvss_score_page_soup.select("title")
                title = str(title).split('<')[1].split('>')[1]  # to remove <title></tile> tags
                cve_id = cve_link_tmp.split('/')[3]
                score = severity.strip().split()[0]
                score = float(score)
        except:
            pass

    if severity_high:
        high_count += 1
        # Store data in dict
        dict_tmp['cve-id'] = cve_id
        dict_tmp['score'] = score
        dict_tmp['link'] = cvss_url
        dict_tmp['title'] = title
        dict_tmp['summary'] = summary
        cve_dict_list.append(dict_tmp)
    i += 1

print(f'All CVE severity checks completed. High:{high_count} Total: {total_cve} ')

sorted_cve_list = sorted(cve_dict_list, key=lambda score: float(score['score']))

with open('cves.log', 'w+') as logfile:
    logfile.write('')
    for line in sorted_cve_list:
        logfile.write(json.dumps(line))
        logfile.write('\n')

finish_time = datetime.now()
exec_time = finish_time-start_time
print(f'Execution time: {exec_time}')
